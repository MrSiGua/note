## 数据分析重头戏之数据整理

**数据整理**，英文名称data munging，是指在获取到的原数据的基础上，理解这些业务数据，整理清洗他们，作为接下来算法建模的输入数据。

### 理解你的业务数据

我们在拿到需要分析的数据后，千万不要急于立刻开始做回归、分类、聚类分析。

第一步应该是认真理解业务数据，可以试着去理解每个特征，观察每个特征，理解它们对结果的影响程度。

然后慢慢研究多个特征组合后，他们对结果的影响。借助常用的统计学指标，比如四分位，绘制箱形图，可以帮助我们寻找样本的取值分布。

同时可以借助另一个强大的可视化工具：seaborn，绘制每个特征变量间的相关系数热图heatmap，帮助我们更好的理解数据。

### 明确各个特征的类型

明确我们的数据类型，也是数据整理阶段的必备任务之一。

如果这些数据类型不是算法部分期望的数据类型，你还得想办法编码成想要的。比如常见的数据自增列id这类数据，是否有必要放到你的算法模型中，因为这类数字很可能被当做数字读入。

某些列的取值类型，虽然已经是数字了，他们的取值大小表示什么含义你也要仔细捉摸。因为，数字的相近相邻，并不一定代表另一种层面的相邻。

有些列是类别型变量（categorical variable），例如注明的Kaggle泰坦尼克还在预测比赛中，乘客上传地点Embarked这个变量就是类别型变量。如果给Embarked变量用Embarked编码为1、2、3，这样的编码是不合理的。

一般这种类型的编码方式有one-hot编码，dummy variable两种方式。

### 找出异常数据

有些时候我们的数据存在异常值，并且这种概率挺大的。这实际上会导致结果出现偏差。比如，统计中国家庭人均收入时，如果源数据里面，有王健林、马云等这种富豪，那么，人均收入的均值就会收到极大的影响，这时候最好，绘制箱形图，看一看百分位数。

了解数据范围，设定最大值、最小值限度是非常重要的。

### 不得不面对缺失值

现实生产环境中，那到的数据恰好完整无损、没有任何缺失数据的概率和买彩票中奖的概率差不多。

数据缺失的原因太多了，业务系统版本迭代，之前的某些字段不再使用了，自然他们的取值就变为null了；再或者，压根某些数据字段在抽样周期里，就是没有写入数据......

处理缺失数据，最好弄明白他们为什么缺失了，比如，像上面说到的，如果实在抽样周期里这些字段取值缺失了，那么可以资讯业务人员，这些字段大概率会取得哪些值。

接下来，填充缺失数据，比如均值填充，或者，为缺失的数据创建一类特殊值。

极端情况下，如果发现模型的效果受此字段影响较大，发现彻底删除此字段效果更好，那完全剔除可能是不错的选择。不过这样做也有风险，可能为模型带来更大的偏差。

### 令人头疼的数据不均衡

理论和实际总是有差距的，理论上很多算法都存在一个基本假设，即数据分布总是均匀的。这个美好的假设，在实际中，真的存在吗？很可能不是！

算法急于不均衡的数据学习出来的模型，在实际的预测集上，效果往往差于训练集上的效果。实际数据往往分部得很不均匀，存在所谓的`“长尾现象”`，又称：`“二八原理”`。

就不均衡解决的难易程度而言，数据量越大，不均衡的问题越容易解决，相反，数据量很小，再不均衡，解决起来就比较困难了，比如典型的太空中是否有生命迹象这个事情，压根就没有太多的相关因素数据，如果某个特征的取值远多于另外一种，处理数据存在不均衡问题，就比较困难了。

所有以上 5 个方面的问题，对于一个数据分析师或数据科学家而言，都是需要认真处理对待的。当然，数据分析完成数据整理后，接下来的主要任务：`特征工程`，也是非常重要的。

